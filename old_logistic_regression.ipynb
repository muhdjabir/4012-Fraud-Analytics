{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.feature_selection import RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350000\n",
      "50000\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv', sep = ',')\n",
    "validate = pd.read_csv('validate.csv', sep = ',')\n",
    "test = pd.read_csv('test.csv', sep = ',')\n",
    "\n",
    "print(len(train))\n",
    "print(len(validate))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run logistic regression on the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision over training dataset: 0.5312\n",
      "Precision over validation dataset: 0.5320\n",
      "Precision over test dataset: 0.5124\n",
      "Recall over training dataset: 0.9857\n",
      "Recall over validation dataset: 0.9823\n",
      "Recall over test dataset: 0.9840\n",
      "F1 score over training dataset: 0.6904\n",
      "F1 score over validation dataset: 0.6902\n",
      "F1 score over test dataset: 0.6739\n"
     ]
    }
   ],
   "source": [
    "feature_cols = [col for col in train.columns if (col != 'fraud')]\n",
    "target_col = ['fraud']\n",
    "X_train = train[feature_cols].copy()\n",
    "y_train = train[target_col].copy()\n",
    "X_validate = validate[feature_cols].copy()\n",
    "y_validate = validate[target_col].copy()\n",
    "X_test  = test[feature_cols].copy()\n",
    "y_test  = test[target_col].copy()\n",
    "\n",
    "LogR = LogisticRegression(random_state=42, class_weight='balanced', max_iter=2000) # for imbalanced learning\n",
    "LogR.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# Calculate precision, recall and f1-score for the training dataset\n",
    "precision_train = precision_score(y_train, LogR.predict(X_train))\n",
    "recall_train = recall_score(y_train, LogR.predict(X_train))\n",
    "f1_score_train = f1_score(y_train, LogR.predict(X_train))\n",
    "\n",
    "# Calculate precision, recall and f1-score for the validation dataset\n",
    "precision_validate = precision_score(y_validate, LogR.predict(X_validate))\n",
    "recall_validate = recall_score(y_validate, LogR.predict(X_validate))\n",
    "f1_score_validate = f1_score(y_validate, LogR.predict(X_validate))\n",
    "\n",
    "# Calculate precision, recall and f1-score for the test dataset\n",
    "precision_test = precision_score(y_test, LogR.predict(X_test))\n",
    "recall_test = recall_score(y_test, LogR.predict(X_test))\n",
    "f1_score_test = f1_score(y_test, LogR.predict(X_test))\n",
    "\n",
    "print('Precision over training dataset: {:0.4f}'.format(precision_train))\n",
    "print('Precision over validation dataset: {:0.4f}'.format(precision_validate))\n",
    "print('Precision over test dataset: {:0.4f}'.format(precision_test))\n",
    "print('Recall over training dataset: {:0.4f}'.format(recall_train))\n",
    "print('Recall over validation dataset: {:0.4f}'.format(recall_validate))\n",
    "print('Recall over test dataset: {:0.4f}'.format(recall_test))\n",
    "print('F1 score over training dataset: {:0.4f}'.format(f1_score_train))\n",
    "print('F1 score over validation dataset: {:0.4f}'.format(f1_score_validate))\n",
    "print('F1 score over test dataset: {:0.4f}'.format(f1_score_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new features that might be useful\n",
    "- attempted_scan: attempted (1) or did not attempt (0)\n",
    "- made_modification: made modification (1) or did not make any modification (0)\n",
    "- no. of items scanned = totalScanTimeInSeconds * scannedLineItemsPerSecond\n",
    "- avg value of each item = grandTotal / no. of items scanned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['attempted_scan'] = np.where(train['scansWithoutRegistration'] > 0, 1, 0)\n",
    "validate['attempted_scan'] = np.where(validate['scansWithoutRegistration'] > 0, 1, 0)\n",
    "test['attempted_scan'] = np.where(test['scansWithoutRegistration'] > 0, 1, 0)\n",
    "\n",
    "train['made_modification'] = np.where(train['quantityModifications'] > 0, 1, 0)\n",
    "validate['made_modification'] = np.where(validate['quantityModifications'] > 0, 1, 0)\n",
    "test['made_modification'] = np.where(test['quantityModifications'] > 0, 1, 0)\n",
    "\n",
    "train['totalItems'] = train['totalScanTimeInSeconds'] * train['scannedLineItemsPerSecond']\n",
    "validate['totalItems'] = validate['totalScanTimeInSeconds'] * validate['scannedLineItemsPerSecond']\n",
    "test['totalItems'] = test['totalScanTimeInSeconds'] * test['scannedLineItemsPerSecond']\n",
    "\n",
    "train['value_per_item'] = train['grandTotal'] / train['total_items_scanned']\n",
    "validate['value_per_item'] = validate['grandTotal'] / validate['total_items_scanned']\n",
    "test['value_per_item'] = test['grandTotal'] / test['total_items_scanned']\n",
    "\n",
    "train[\"total_cost\"] = train[\"totalScanTimeInSeconds\"] * train[\"valuePerSecond\"]\n",
    "validate[\"total_cost\"] = validate[\"totalScanTimeInSeconds\"] * validate[\"valuePerSecond\"]\n",
    "test[\"total_cost\"] = test[\"totalScanTimeInSeconds\"] * test[\"valuePerSecond\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trustLevel                   -0.000681\n",
      "totalScanTimeInSeconds       -0.000135\n",
      "grandTotal                   -0.002165\n",
      "lineItemVoids                 0.001811\n",
      "scansWithoutRegistration      0.001743\n",
      "quantityModifications         0.000359\n",
      "scannedLineItemsPerSecond    31.774127\n",
      "valuePerSecond               31.442346\n",
      "lineItemVoidsPerPosition      4.535840\n",
      "attempted_scan               -2.852944\n",
      "made_modification            -1.784765\n",
      "total_items_scanned          -0.000936\n",
      "value_per_item                4.467149\n",
      "total_cost                   -0.002165\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# check skewness of data\n",
    "feature_cols = [col for col in train.columns if (col != 'fraud')]\n",
    "target_col = ['fraud']\n",
    "X_train = train[feature_cols].copy()\n",
    "y_train = train[target_col].copy()\n",
    "X_validate = validate[feature_cols].copy()\n",
    "y_validate = validate[target_col].copy()\n",
    "X_test  = test[feature_cols].copy()\n",
    "y_test  = test[target_col].copy()\n",
    "\n",
    "print(X_train.skew(axis = 0, skipna = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results, we can see that `scannedLineItemsPerSecond`, `valuePerSecond`, and `lineItemVoidsPerPosition`, `value_per_item` are positively skewed. We can then do log transformation on these features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.loc[:, 'scannedLineItemsPerSecond'] = np.log(X_train.loc[:, 'scannedLineItemsPerSecond'] + 1)\n",
    "X_train.loc[:, 'valuePerSecond'] = np.log(X_train.loc[:, 'valuePerSecond'] + 1)\n",
    "X_train.loc[:, 'lineItemVoidsPerPosition'] = np.log(X_train.loc[:, 'lineItemVoidsPerPosition'] + 1)\n",
    "X_train.loc[:, 'value_per_item'] = np.log(X_train.loc[:, 'value_per_item'] + 1)\n",
    "\n",
    "X_validate.loc[:, 'scannedLineItemsPerSecond'] = np.log(X_validate.loc[:, 'scannedLineItemsPerSecond'] + 1)\n",
    "X_validate.loc[:, 'valuePerSecond'] = np.log(X_validate.loc[:, 'valuePerSecond'] + 1)\n",
    "X_validate.loc[:, 'lineItemVoidsPerPosition'] = np.log(X_validate.loc[:, 'lineItemVoidsPerPosition'] + 1)\n",
    "X_validate.loc[:, 'value_per_item'] = np.log(X_validate.loc[:, 'value_per_item'] + 1)\n",
    "\n",
    "X_test.loc[:, 'scannedLineItemsPerSecond'] = np.log(X_test.loc[:, 'scannedLineItemsPerSecond'] + 1)\n",
    "X_test.loc[:, 'valuePerSecond'] = np.log(X_test.loc[:, 'valuePerSecond'] + 1)\n",
    "X_test.loc[:, 'lineItemVoidsPerPosition'] = np.log(X_test.loc[:, 'lineItemVoidsPerPosition'] + 1)\n",
    "X_test.loc[:, 'value_per_item'] = np.log(X_test.loc[:, 'value_per_item'] + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform min-max scaling on the other continuous features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cols = [col for col in train.columns if \n",
    "            (col != 'fraud') and (col != 'attempted_scan') and (col != 'made_modification') and (col != 'trustLevel') and\n",
    "            (col != 'scannedLineItemsPerSecond') and (col != 'valuePerSecond') and (col != 'lineItemVoidsPerPosition') and\n",
    "            (col != 'value_per_item')]\n",
    "\n",
    "# Create a MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the selected features in the train, validate, and test sets\n",
    "train[all_cols] = scaler.fit_transform(train[all_cols])\n",
    "validate[all_cols] = scaler.transform(validate[all_cols])\n",
    "test[all_cols] = scaler.transform(test[all_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run logistic regression again on ALL features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision over training dataset: 0.7247\n",
      "Precision over validation dataset: 0.7268\n",
      "Precision over test dataset: 0.7088\n",
      "Recall over training dataset: 0.9958\n",
      "Recall over validation dataset: 0.9971\n",
      "Recall over test dataset: 0.9961\n",
      "F1 score over training dataset: 0.8389\n",
      "F1 score over validation dataset: 0.8408\n",
      "F1 score over test dataset: 0.8282\n"
     ]
    }
   ],
   "source": [
    "feature_cols = [col for col in train.columns if (col != 'fraud')]\n",
    "target_col = ['fraud']\n",
    "X_train = train[feature_cols].copy()\n",
    "y_train = train[target_col].copy()\n",
    "X_validate = validate[feature_cols].copy()\n",
    "y_validate = validate[target_col].copy()\n",
    "X_test  = test[feature_cols].copy()\n",
    "y_test  = test[target_col].copy()\n",
    "\n",
    "LogR = LogisticRegression(random_state=42, class_weight='balanced', max_iter=2000) # for imbalanced learning\n",
    "LogR.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# Calculate precision, recall and f1-score for the training dataset\n",
    "precision_train = precision_score(y_train, LogR.predict(X_train))\n",
    "recall_train = recall_score(y_train, LogR.predict(X_train))\n",
    "f1_score_train = f1_score(y_train, LogR.predict(X_train))\n",
    "\n",
    "# Calculate precision, recall and f1-score for the validation dataset\n",
    "precision_validate = precision_score(y_validate, LogR.predict(X_validate))\n",
    "recall_validate = recall_score(y_validate, LogR.predict(X_validate))\n",
    "f1_score_validate = f1_score(y_validate, LogR.predict(X_validate))\n",
    "\n",
    "# Calculate precision, recall and f1-score for the test dataset\n",
    "precision_test = precision_score(y_test, LogR.predict(X_test))\n",
    "recall_test = recall_score(y_test, LogR.predict(X_test))\n",
    "f1_score_test = f1_score(y_test, LogR.predict(X_test))\n",
    "\n",
    "print('Precision over training dataset: {:0.4f}'.format(precision_train))\n",
    "print('Precision over validation dataset: {:0.4f}'.format(precision_validate))\n",
    "print('Precision over test dataset: {:0.4f}'.format(precision_test))\n",
    "print('Recall over training dataset: {:0.4f}'.format(recall_train))\n",
    "print('Recall over validation dataset: {:0.4f}'.format(recall_validate))\n",
    "print('Recall over test dataset: {:0.4f}'.format(recall_test))\n",
    "print('F1 score over training dataset: {:0.4f}'.format(f1_score_train))\n",
    "print('F1 score over validation dataset: {:0.4f}'.format(f1_score_validate))\n",
    "print('F1 score over test dataset: {:0.4f}'.format(f1_score_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove highly correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highly correlated variable pairs:\n",
      "valuePerSecond and scannedLineItemsPerSecond have a correlation of 0.7472137111808428\n",
      "total_cost and grandTotal have a correlation of 0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have already calculated the correlation matrix\n",
    "correlation_matrix_train = train.corr()\n",
    "\n",
    "# Set a threshold for high correlation\n",
    "threshold = 0.7  # You can adjust this threshold as needed\n",
    "\n",
    "# Find highly correlated pairs of variables\n",
    "highly_correlated_pairs = set()\n",
    "for i in range(len(correlation_matrix_train.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix_train.iloc[i, j]) > threshold:\n",
    "            col1 = correlation_matrix_train.columns[i]\n",
    "            col2 = correlation_matrix_train.columns[j]\n",
    "            highly_correlated_pairs.add((col1, col2))\n",
    "\n",
    "# Print the highly correlated variable pairs\n",
    "print(\"Highly correlated variable pairs:\")\n",
    "for pair in highly_correlated_pairs:\n",
    "    print(f\"{pair[0]} and {pair[1]} have a correlation of {correlation_matrix_train.loc[pair[0], pair[1]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove each and see the difference it makes to the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing valuePerSecond only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision over training dataset: 0.7246\n",
      "Precision over validation dataset: 0.7270\n",
      "Precision over test dataset: 0.7091\n",
      "Recall over training dataset: 0.9958\n",
      "Recall over validation dataset: 0.9971\n",
      "Recall over test dataset: 0.9961\n",
      "F1 score over training dataset: 0.8388\n",
      "F1 score over validation dataset: 0.8409\n",
      "F1 score over test dataset: 0.8284\n"
     ]
    }
   ],
   "source": [
    "feature_cols = [col for col in train.columns if (col != 'fraud') and (col != 'valuePerSecond')]\n",
    "target_col = ['fraud']\n",
    "X_train = train[feature_cols].copy()\n",
    "y_train = train[target_col].copy()\n",
    "X_validate = validate[feature_cols].copy()\n",
    "y_validate = validate[target_col].copy()\n",
    "X_test  = test[feature_cols].copy()\n",
    "y_test  = test[target_col].copy()\n",
    "\n",
    "LogR = LogisticRegression(random_state=42, class_weight='balanced', max_iter=2000) # for imbalanced learning\n",
    "LogR.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# Calculate precision, recall and f1-score for the training dataset\n",
    "precision_train = precision_score(y_train, LogR.predict(X_train))\n",
    "recall_train = recall_score(y_train, LogR.predict(X_train))\n",
    "f1_score_train = f1_score(y_train, LogR.predict(X_train))\n",
    "\n",
    "# Calculate precision, recall and f1-score for the validation dataset\n",
    "precision_validate = precision_score(y_validate, LogR.predict(X_validate))\n",
    "recall_validate = recall_score(y_validate, LogR.predict(X_validate))\n",
    "f1_score_validate = f1_score(y_validate, LogR.predict(X_validate))\n",
    "\n",
    "# Calculate precision, recall and f1-score for the test dataset\n",
    "precision_test = precision_score(y_test, LogR.predict(X_test))\n",
    "recall_test = recall_score(y_test, LogR.predict(X_test))\n",
    "f1_score_test = f1_score(y_test, LogR.predict(X_test))\n",
    "\n",
    "print('Precision over training dataset: {:0.4f}'.format(precision_train))\n",
    "print('Precision over validation dataset: {:0.4f}'.format(precision_validate))\n",
    "print('Precision over test dataset: {:0.4f}'.format(precision_test))\n",
    "print('Recall over training dataset: {:0.4f}'.format(recall_train))\n",
    "print('Recall over validation dataset: {:0.4f}'.format(recall_validate))\n",
    "print('Recall over test dataset: {:0.4f}'.format(recall_test))\n",
    "print('F1 score over training dataset: {:0.4f}'.format(f1_score_train))\n",
    "print('F1 score over validation dataset: {:0.4f}'.format(f1_score_validate))\n",
    "print('F1 score over test dataset: {:0.4f}'.format(f1_score_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing scannedLineItemsPerSecond only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision over training dataset: 0.7241\n",
      "Precision over validation dataset: 0.7255\n",
      "Precision over test dataset: 0.7082\n",
      "Recall over training dataset: 0.9958\n",
      "Recall over validation dataset: 0.9971\n",
      "Recall over test dataset: 0.9961\n",
      "F1 score over training dataset: 0.8385\n",
      "F1 score over validation dataset: 0.8399\n",
      "F1 score over test dataset: 0.8278\n"
     ]
    }
   ],
   "source": [
    "feature_cols = [col for col in train.columns if (col != 'fraud') and (col != 'scannedLineItemsPerSecond')]\n",
    "target_col = ['fraud']\n",
    "X_train = train[feature_cols].copy()\n",
    "y_train = train[target_col].copy()\n",
    "X_validate = validate[feature_cols].copy()\n",
    "y_validate = validate[target_col].copy()\n",
    "X_test  = test[feature_cols].copy()\n",
    "y_test  = test[target_col].copy()\n",
    "\n",
    "LogR = LogisticRegression(random_state=42, class_weight='balanced', max_iter=2000) # for imbalanced learning\n",
    "LogR.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# Calculate precision, recall and f1-score for the training dataset\n",
    "precision_train = precision_score(y_train, LogR.predict(X_train))\n",
    "recall_train = recall_score(y_train, LogR.predict(X_train))\n",
    "f1_score_train = f1_score(y_train, LogR.predict(X_train))\n",
    "\n",
    "# Calculate precision, recall and f1-score for the validation dataset\n",
    "precision_validate = precision_score(y_validate, LogR.predict(X_validate))\n",
    "recall_validate = recall_score(y_validate, LogR.predict(X_validate))\n",
    "f1_score_validate = f1_score(y_validate, LogR.predict(X_validate))\n",
    "\n",
    "# Calculate precision, recall and f1-score for the test dataset\n",
    "precision_test = precision_score(y_test, LogR.predict(X_test))\n",
    "recall_test = recall_score(y_test, LogR.predict(X_test))\n",
    "f1_score_test = f1_score(y_test, LogR.predict(X_test))\n",
    "\n",
    "print('Precision over training dataset: {:0.4f}'.format(precision_train))\n",
    "print('Precision over validation dataset: {:0.4f}'.format(precision_validate))\n",
    "print('Precision over test dataset: {:0.4f}'.format(precision_test))\n",
    "print('Recall over training dataset: {:0.4f}'.format(recall_train))\n",
    "print('Recall over validation dataset: {:0.4f}'.format(recall_validate))\n",
    "print('Recall over test dataset: {:0.4f}'.format(recall_test))\n",
    "print('F1 score over training dataset: {:0.4f}'.format(f1_score_train))\n",
    "print('F1 score over validation dataset: {:0.4f}'.format(f1_score_validate))\n",
    "print('F1 score over test dataset: {:0.4f}'.format(f1_score_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run RFE to choose the best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True  True  True False  True  True  True  True  True  True]\n",
      "['trustLevel', 'totalScanTimeInSeconds', 'grandTotal', 'lineItemVoids', 'scansWithoutRegistration', 'scannedLineItemsPerSecond', 'lineItemVoidsPerPosition', 'attempted_scan', 'made_modification', 'total_items_scanned', 'value_per_item']\n"
     ]
    }
   ],
   "source": [
    "feature_cols = [col for col in train.columns if (col != 'fraud') and (col != 'valuePerSecond')]\n",
    "target_col = ['fraud']\n",
    "X_train = train[feature_cols].copy()\n",
    "y_train = train[target_col].copy()\n",
    "X_validate = validate[feature_cols].copy()\n",
    "y_validate = validate[target_col].copy()\n",
    "X_test  = test[feature_cols].copy()\n",
    "y_test  = test[target_col].copy()\n",
    "\n",
    "LogR = LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000) # for imbalanced learning\n",
    "\n",
    "# Use RFE to select the top 10 features\n",
    "rfe = RFE(LogR, n_features_to_select=11)\n",
    "rfe.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# Print the selected features\n",
    "print(rfe.support_)\n",
    "\n",
    "# Get the selected feature indexes\n",
    "selected_feature_indexes = [i for i, selected in enumerate(rfe.support_) if selected]\n",
    "selected_features = [feature_cols[i] for i in selected_feature_indexes]\n",
    "print(selected_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the selected features, run the logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision over training dataset: 0.7183\n",
      "Precision over validation dataset: 0.7184\n",
      "Precision over test dataset: 0.7054\n",
      "Recall over training dataset: 0.9955\n",
      "Recall over validation dataset: 0.9979\n",
      "Recall over test dataset: 0.9954\n",
      "F1 score over training dataset: 0.8345\n",
      "F1 score over validation dataset: 0.8354\n",
      "F1 score over test dataset: 0.8257\n"
     ]
    }
   ],
   "source": [
    "selected_features = ['trustLevel', 'lineItemVoids', 'scansWithoutRegistration', 'totalScanTimeInSeconds', 'grandTotal', 'total_items_scanned']\n",
    "\n",
    "feature_cols = [col for col in selected_features]\n",
    "target_col = ['fraud']\n",
    "X_train = train[feature_cols].copy()\n",
    "y_train = train[target_col].copy()\n",
    "X_validate = validate[feature_cols].copy()\n",
    "y_validate = validate[target_col].copy()\n",
    "X_test  = test[feature_cols].copy()\n",
    "y_test  = test[target_col].copy()\n",
    "\n",
    "LogR = LogisticRegression(random_state=42, class_weight='balanced', max_iter=2000) # for imbalanced learning\n",
    "LogR.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# Calculate precision, recall and f1-score for the training dataset\n",
    "precision_train = precision_score(y_train, LogR.predict(X_train))\n",
    "recall_train = recall_score(y_train, LogR.predict(X_train))\n",
    "f1_score_train = f1_score(y_train, LogR.predict(X_train))\n",
    "\n",
    "# Calculate precision, recall and f1-score for the validation dataset\n",
    "precision_validate = precision_score(y_validate, LogR.predict(X_validate))\n",
    "recall_validate = recall_score(y_validate, LogR.predict(X_validate))\n",
    "f1_score_validate = f1_score(y_validate, LogR.predict(X_validate))\n",
    "\n",
    "# Calculate precision, recall and f1-score for the test dataset\n",
    "precision_test = precision_score(y_test, LogR.predict(X_test))\n",
    "recall_test = recall_score(y_test, LogR.predict(X_test))\n",
    "f1_score_test = f1_score(y_test, LogR.predict(X_test))\n",
    "\n",
    "print('Precision over training dataset: {:0.4f}'.format(precision_train))\n",
    "print('Precision over validation dataset: {:0.4f}'.format(precision_validate))\n",
    "print('Precision over test dataset: {:0.4f}'.format(precision_test))\n",
    "print('Recall over training dataset: {:0.4f}'.format(recall_train))\n",
    "print('Recall over validation dataset: {:0.4f}'.format(recall_validate))\n",
    "print('Recall over test dataset: {:0.4f}'.format(recall_test))\n",
    "print('F1 score over training dataset: {:0.4f}'.format(f1_score_train))\n",
    "print('F1 score over validation dataset: {:0.4f}'.format(f1_score_validate))\n",
    "print('F1 score over test dataset: {:0.4f}'.format(f1_score_test))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
